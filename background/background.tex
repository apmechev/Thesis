\chapter{Background Theory}

\label{ch:background}

\section{Introduction}

Astronomy is rapidly entering the big data regime with many large-scale sky surveys across the electromagnetic spectrum. This breadth of information is poised to expand the frontiers of Astronomy and Astrophysics and allow us to study and understand various phenomena in more detail. The longest wavelength of the spectrum accessible to Earth observatories lies in the Megahertz range, starting at 10 MHz, up to a Gigahertz, corresponding to wavelengths of 30 meters up to 30 cm. In Astronomy, this range is termed the low-frequency or meter-wave regime. These wavelengths help uncover physical phenomena not accessible to projects in the X-ray, Visible, Infrared or Microwave. In particular, long-wavelengths can be used to study supermassive black holes, galaxy formation and evolution, magneto-hydrodynamics, solar physics, radio spectroscopy and many more science cases.

Photons provide us with rather few independent properties. Light provides us with the wavelength, the direction, the intensity  and the polarization of a distant source, as well as the change of those properties with time. Astronomers use these properties to better model distant sources, and validate or reject astronomical theories. The accuracy of these models, or the rejection power of our observations depend critically on how accurately we can measure the four properties listed above. 

Astronomical observations in the long-wavelength regime have always been at the mercy of the diffraction limit, an effect that relates the wavelength of light, the diameter of the aperture and angular resolution obtained with that aperture. The angular resolution of a telescope determines how accurately the direction of an incoming photon is determined. Unfortunately the diffraction limit dictates that the angular resolution of a telescope with a fixed aperture decreases inversely proportional to the wavelength observed. For example, for a telescope at 100MHz to reach the same resolution as the 100-m diameter Effelsberg telescope at 10GHz, the 100MHz telescope would need a diameter of a 10 kilometers. Constructing, and operating a telescope of that size is currently outside our engineering capabilities, and thus low frequency astronomers have developed a method to synthesize a telescope aperture of an arbitrary size. 

Aperture synthesis is the practice of combining the signal of multiple antennas to produce data with the angular resolution of a much larger antenna. More specifically, the maximum angular resolution achievable is related to the distance between your furthest two antennas. This technique has been used in a wide wavelength range, from the near- and mid-infrared (VLTI), sub-millimeter (ALMA) and radio wavelengths (VLA, GMRT, LWA). While this method is useful to increase the angular resolution of a telescope, it also requires significant post-observation computation in order to remove artifacts created by the synthetic aperture. In this work we aim to introduce LOFAR, the European Low Frequency Array, the data sizes and processing challenges that come with LOFAR data as well as our solutions to these challenges. We will conclude with the scientific results this work has led to, as well as suggestions for future large-scale astronomical projects

\subsection{LOFAR}

LOFAR is a large low-frequency radio telescope centered near Dwingeloo, Drenthe, in the Netherlands. In the Netherlands, LOFAR has thousands of antennas grouped in Core (near Dwingeloo) and Remote stations.LOFAR also has International stations across Europe, spanning from Ireland to Estonia, Sweden to Italy. These international stations make it possible to create images of radio sources with similar angular resolution to leading higher frequency telescopes. Much like these telescopes, LOFAR was also designed to support a variety of science cases, from large scale broadband studies, to spectroscopy and transient detection. 

LOFAR stores its broadband data at one of several Long-Term Archive locations. These locations store the data on tape, due to its large size and infrequent access. Typical broadband observations are up to 16TB in size, which can drop down to 6TB with compression. While individual researchers use this data to study their object of interest, the majority of the broadband data will be imaged to produce the LOFAR Two-Meter Sky Survey (LoTSS). 

\subsection{LoTSS} 

The LOFAR Two-Meter Sky Survey, LoTSS, is an ambitious project to map the Northern Radio sky at low-wavelengths namely 120-168 MHz. Expected to produce more than 3000 8-hour observations, LoTSS will create radio maps with sensitivity below 100 $\mu$Jy/beam. This survey will help study supermassive black holes and their impact on galaxy formation in the early Universe. Additionally, understanding the formation and evolution of galactic clusters and the interaction of galaxies within these clusters will be made possible with this low-frequency data. Furthermore, the survey will enable us to study star formation in nearby and distant galaxies and galactic sources such as supernova remnants. Finally, LoTSS will help study and discover patterns in the large-scale structure of the Universe.  

The observations produced by LoTSS will total more than 30 petabytes and require extensive processing before the survey is completed. Each observation is stored as a set of 244 individual files spanning frequency space from 120MHz to 168MHz. Each of these files, named a Subband is a CASA Measurement set and is identified by its three digit Subband number, starting from 000. Each Subband, thus, contains a sub-sample of the data in frequency space, stored at a resolution of 1 second and 12.2 kHz per sample. While this high-resolution data is useful for some science cases, our processing algorithms scale with data size, and thus it is necessary to average our data in order to complete the LoTSS processing within the project's time-frame. 

In order to create an image from an archived data set, the data needs to be staged, retrieved and processed. Staging the data refers to sending a request to the archive site to move the data from tape to disk. Once all your data is on disk ('staged'), it is ready to be transferred from the storage to the processing cluster. On this cluster, a science-ready image is produced by processing the raw data through two pipelines. The first pipeline, Direction Independent Calibration pipeline removes artifacts created by `direction independent' effects, i.e. effects that are constant across the field of interest. This pipeline is followed by the Direction Dependent Calibration pipeline which removes effects that change within the field of view. 

The Direction Independent Calibration pipeline (DI pipeline) consists of two main stages. The first stage is calibration on the calibrator, which uses a short observation of a bright calibration source to determine systematic effects that are independent of the direction of the pointing. The solutions obtained from this step can be applied to the scientific target, improving the data quality. 
