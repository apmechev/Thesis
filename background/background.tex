\section{Introduction}\label{sec:intro_intro}

For almost a century, computational machines have been used as a tool to improve scientific research. In part driven by national security concerns during the First and Second World Wars as well as the Cold War, increasingly complex computers have been designed. Early computers were entirely designed for application specific tasks, with a large drive behind them being nuclear simulations for the Manhattan project. In 1945, the popular Von-Neumann architecture was developed with the goal of making Monte Carlo simulations easier to develop and to facilitate general purpose computing. This architecture was a significant improvement over previous computers where changing the program reuqired physically flipping switches and changing cables on the computer itself. One of the first computers built according to the von-Neumann architecture is the MANIAC computer comissioned by Los Alamos national Laboratory, seen on the left in Figure \ref{fig:intro_supercomputers}.

With the advancement of an architecture that treats code and data identically, it was possible to create more complex programs including compilers: programs that could creade machine code from human readable code. As the 50s and 60s passed, general computers were increasingly used in science. From weather dynamics to fluid dynamics, from chaos theory to game theory, these computers were being adopted by a wide range of scientific fields. Astronomy was likewise also a driving force for computational innovation. In 1953, for example, the first high level programming language for IBM computers was developed by John Backus, a programmer frustrated with the difficulty of accurately calculating the moon's position using only machine code. John Backus' `Speedcode' was a direct predecessor of Fortran, a language developed at IBM in the '50s and still used by the scientific community today. Another important discovery on our road was the Fast Fourier Transform (FFT), discovered by two researchers from Princeton and IBM. The FFT has been described as `the most important numerical algorithm of our lifetime' and the author's personal favourite `an algorithm the whole family can use'\citep{top_10_algos}. 

\begin{figure}[h]
\includegraphics[width=.45\linewidth]{background/figures/Maniac_1952.jpg}\quad\includegraphics[width=.45\linewidth]{background/figures/cartesius.jpeg}
    \caption[Two supercomputers sixty years apart.]{Two supercomputers sixty years apart. On the left is the MANIAC computer from 1952, while on the right is the Cartesius cluster at SURFsara, in 2018.}
    \label{fig:intro_supercomputers}

\end{figure}

As computers became more available, they were increasingly adopted by universities and research institutes. In the '70s, computers began to talk to each other over a network connection. This capability not only made scientific collaboration easier, but made it possible to split computation across multiple sites. Moreover, the development of the integrated circuit and subsequent drop of price/performance of computers made it financially feasible for scientific institutes to purchase multiple computers dedicated to scientific research. As hardware, networks and software matured, computer clusters became more widely used\citep{tel_2000}. In part because of their cost effectiveness, and potential for parallelization, clusters of computers became more widely used as the 80's wound down. By then, general purpose computing was widely adopted by the astronomical community, with software such as AIPS and IRAF and standards such as FITS used to this day. 

The 90's continued the distributed computing trend with the appearance of commodity compute clusters, with virtual `supercomputers' being created from Commercial Off-The Shelf (COTS) hardware, and networking. These clusters became quickly adopted by scientists to perform simulations and data processing. Around the same time, the idea of 'grids' was created. The concept was that of a country, continent or even world-wide network of hardware that can transparently handle distributed tasks, and provide researchers with a vast pool of resources. Grid processing was pioneered by CERN with the goal of meeting the computational and storage requirements of their High Energy Physics modelling and data reduction. 


\subsection{Astronomy and Computing}

Since the early days of computing, the field of astronomy has embraced digitization of data acquisition and processing. Being able to store astronomical data digitally makes it possible to transfer, copy, backup and process them easily. With the rapid development of CCDs, optical astronomy entered the digital age, however with the wide availability of Analog-Digital Converters (ADCs), radio astronomy has been digital since the 1970s. By the end of the 70's, the Very Large Array (VLA) in New Mexico and the Westerbork Synthesis Radio Telescope (WSRT) had consistenlty been using processing pipelines with imaging algorithms taking advantage of the FFT developed a decade earlier\citep{clark1980_clean}, running on Digital Equipment Corporation's line of PDP, and later VAX, minicomputers. 

With the complete digitization of astronomical observations, over the past decade,  all of astronomy has entered the big data regime. As of 2019, there are multiple planned and ongoing large-scale sky surveys across the electromagnetic spectrum, each expecting to produce multiple tens of petabytes. This breadth of data is poised to expand the frontiers of astronomy and astrophysics and allow us to study and understand various phenomena in more detail.

The longest wavelength of the spectrum accessible to Earth observatories lies in the Megahertz range, starting at 10 MHz, up to 300 MHz. This regime corresponding to wavelengths of 30 meters up to 1 meter. In astronomy, this range is termed the low-frequency or meter-wave regime. These wavelengths help uncover physical phenomena invisible to telescopes in the X-ray, Visible, Infrared or Microwave. In particular, long-wavelengths can be used to study supermassive black holes, galaxy formation and evolution, magneto-hydrodynamics, solar physics, radio spectroscopy and many more science cases. Additionally, data in this domain can complement other telescopes in multi-wavelength studies.

Photons provide us with rather few independent properties. Light provides us with the wavelength, the direction, the intensity and the polarization of a distant source, as well as the change of those properties with time. Astronomers need to use measure properties accurately and use the data to better model distant sources, and validate or reject astronomical theories. The accuracy of these models, or the rejection power of our observations depend critically on how accurately we can measure the four properties listed above. 

Astronomical observations in the long-wavelength regime have always been at the mercy of the diffraction limit, an effect that relates the wavelength of light, the diameter of the aperture and angular resolution obtained with that aperture. The angular resolution of a telescope determines how accurately the direction of an incoming photon is determined. Unfortunately the diffraction limit dictates that the angular resolution of a telescope with a fixed aperture decreases inversely proportional to the wavelength observed. For example, if you take a telescope at 100MHz and one at 10GHz, the 100MHz telescope would need to have 100 times the radius of its higher frequency counterpart  in order to reach the same angular resolution. In other words, for LOFAR to match the 100-m Effelsberg telescope (at 10GHz), LOFAR would need a dish with a diameter of 10 kilometers. Constructing, and operating a telescope of that size is currently outside our engineering capabilities, and thus low frequency astronomers have developed a method to synthesize a telescope aperture of an arbitrary size, termed `Aperture Synthesis'. 

\begin{figure}[h]
    \centering
    \includegraphics[width=.8\linewidth]{background/figures/apertureSynthesis.png}
    \caption[Graphical representation of aperture synthesis]{We can simulate a single dish with an array of antennas. These antennae are pointed in different directions by introducing a corresponding hardware delay in each antenna feed. While this process can enable us to synthesize an arbitrarily large telescope, it produces artefacts in the final image that need extensive processing to remove. }
    \label{fig:aperture_synthesis}
\end{figure}


Aperture synthesis is the practice of combining the signal of multiple antennas to produce data with the angular resolution of a much larger antenna, as seen in Figure \ref{fig:aperture_synthesis}. More specifically, the maximum angular resolution achievable is related to the distance between your furthest two antennas. This technique has been used in a wide wavelength range, from the near- and mid-infrared (VLTI), sub-millimeter (ALMA) and radio wavelengths (VLA, GMRT, LWA). While this method is useful to increase the angular resolution of a telescope, it also requires significant post-observation computation in order to remove artifacts created by the synthetic aperture. In this work we aim to introduce LOFAR, the European Low Frequency Array, the data sizes and processing challenges that come with LOFAR data as well as our solutions to these challenges. We will conclude with the scientific results this work has led to, as well as suggestions for future large-scale astronomical projects.

\subsection{LOFAR}

LOFAR is a large low-frequency radio telescope centered near Dwingeloo, Drenthe, in the Netherlands. In the Netherlands, LOFAR has thousands of antennas grouped in Core (near Dwingeloo) and Remote stations. LOFAR also has International stations across Europe, spanning from Ireland to Estonia, Sweden to Italy. These international stations make it possible to create images of radio sources with similar angular resolution to leading higher frequency telescopes. Much like these telescopes, LOFAR was also designed to support a variety of science cases, from large scale broadband studies, to spectroscopy and transient detection. 

LOFAR stores its broadband data at one of several Long-Term Archive locations. These locations store the data on tape, due to its large size and infrequent access. Typical broadband observations are up to 16TB in size, which can drop down to 8TB with compression. While individual researchers use this data to study their object of interest, the majority of the broadband data will be imaged to produce the LOFAR Two-Meter Sky Survey (LoTSS). 

\subsubsection{LoTSS} 

The LOFAR Two-Meter Sky Survey, LoTSS, is an ambitious project to map the Northern Radio sky at low frequencies namely 120-168 MHz. Expected to produce more than 3000 8-hour observations, LoTSS will create radio maps with sensitivity below 100 $\mu$Jy/beam. This survey will help study supermassive black holes and their impact on galaxy formation in the early Universe. Additionally, understanding the formation and evolution of galactic clusters and the interaction of galaxies within these clusters will be made possible with this low-frequency data. Furthermore, the survey will enable us to study star formation in nearby and distant galaxies and galactic sources such as supernova remnants. Finally, LoTSS will help study and discover patterns in the large-scale structure of the Universe.  The observations produced by LoTSS will total more than 30 petabytes and require extensive processing before the survey is completed. 

\subsubsection{Processing Requirements}   

With its 3000+ observations, the LoTSS project requires a large amount of processing, bandwidth and storage infrastructure in order to complete its scientific goals within the survey timespan. The total size of raw data is more than 30 petabytes, while the total size of the finished products will be on the order of 10s of terabytes. Furthermore, moving all the raw data to processing facilites is limited by the bandwith of the connection between the archive site and the processing facility. Finally, each data set requires 500 core-hours for the DI pipeline and rougly 3000 core-hours for the DD pipeline. In total this means that the LoTSS project will take more than 10 million core-hours to produce scientific results, assuming no re-processing of data. 
 
In addition to the raw hardware requirements, a large project as such needs to be able to track the status and location of data products, automate processing and make results easily available. As LOFAR data is stored at multiple locations, it is also important that the framework tasked with processing LoTSS data is portable and can run independent of the infrastructure details.

\subsection{SURFsara} 

One of the archive locations storing LOFAR data is SURFsara at the Amsterdam Science Park. Aside from a large storage archive, SURFsara also supports several clusters, including the Gina cluster, part of the Dutch Grid infrastructure. Grid computing is a non-interactive application-oriented computational paradigm for distributed computing where a 'grid' consists of a large pool of nodes where users can submit batch jobs. A grid can consist of one cluster or groups of clusters at one or multiple geographical locations, connected with high-speed links and a common job management interface. Using this interface, users can scale out their projects, given that their processing is massively parallel. Computational resources on such a platform are granted based on a scientific proposal, and are used freely across the Grid, while jobs are scheduled based on the job requirements and the current resource availability of the grid nodes. This processing paradigm is perfect for large grid-search simulations, but also for the first steps of LOFAR processing. Furthermore, the high speed connection to the LOFAR archive makes SURFsara a logical location to orchestrate large scale LOFAR projects.


\subsection{LoTSS Processing}

The observations produced by LoTSS will total more than 30 petabytes and require extensive processing before the survey is completed. Each observation is stored as a set of 244 individual files spanning frequency space from 120MHz to 168MHz. Each of these files, named a Subband is a CASA Measurement set and is identified by its three digit Subband number, starting from 000. Each Subband, thus, contains a sub-sample of the data in frequency space, stored at a resolution of 1 second and 12.2 kHz per sample. While this high-resolution data is useful for some science cases, our processing algorithms scale with data size, and thus it is necessary to average our data in order to complete the LoTSS processing within the project's time-frame. 

In order to create an image from an archived data set, the data needs to be staged, retrieved and processed. Staging the data refers to sending a request to the archive site to move the data from tape to disk. Once all your data is on disk ('staged'), it is ready to be transferred from the storage to the processing cluster. On this cluster, a science-ready image is produced by processing the raw data through two pipelines. The first pipeline, Direction Independent Calibration pipeline removes artifacts created by `direction independent' effects, i.e. effects that are constant across the field of interest. This pipeline is followed by the Direction Dependent Calibration pipeline which removes effects that change within the field of view.
 
The Direction Independent Calibration pipeline (DI pipeline) consists of two main stages. The first stage is calibration on the calibrator, which uses a short observation of a bright calibration source to determine systematic effects that are independent of the direction of the pointing. The solutions obtained from this step can be applied to the scientific target, improving the data quality. The second step of the DI pipeline is the calibration of the target field against a sky model produced by a previous survey. This calibration determines the gain parameters of all antennas, however it does not correct for effects that vary across the field of view. 
 
In order to create a high fidelity radio image, we need to correct for effects that not only change in time, but also across the field of view. These effects, such as the ionosphere or the beam response can be modelled and removed, and their removal is the responsibility of the Direction Dependent pipeline (DD pipeline). Upon successful completion, the DD pipeline produces a radio image that can be used for further scientific studies.

There are a few software packages used to process LOFAR data, typically used in a series of steps creating a processing pipeline. The LOFAR processing pipeline steps use the software, each step encoding the processing parameters in a parameter-set (parset). A pipeline is defined by the list of steps and the parameters of each step concatenated together into a parset file. The LoTSS DI pipeline, \texttt{prefactor} contains a set of scripts used to remove direction independent effects from LOFAR data. Many of the \texttt{prefactor} steps can be executed on the data in parallel: each Subband can be processed independently. Because of the large amount of data, the best architecture for these steps is a cluster of isolated machines with dedicated disks and a high speed connection to the data. The Dutch grid infrastructure at SURFsara is a logical place to launch these steps. 

Our main goal is to provide the software infrastructure to make it easy for scientists to automatically process archived LOFAR data and produce science quality results with minor modifications to their processing scripts.

\subsection{Research questions}

The main research questions addresse in this work are:

How do we create a general framework that can support LOFAR processing across a distributed cluster?

How do we orchestrate complex workflows across distributed infrastructure and include continuous integration to ensure consistent software tools and pipelines?

How do we collect and process performance statistics per pipeline step and use them to better understand the performance of our software?








