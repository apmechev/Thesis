\cleardoublepage
\phantomsection
 
\addcontentsline{toc}{chapter}{English Summary}

\chapter*{English Summary}\label{ch:summary_eng}
When we look up at the night sky, we observe the universe through visible light. We can see stars, planets, nebulae and maybe even other galaxies. The earliest astronomers studied the uiniverse through their eyes, eventually aided by mirrors and lenses in the form of telescopes and binoculars. In the 18th and 19th centuries, physicists have unraveled the breadth of the electromagnetic spectrum. Aside from visible light, astronomers were now able to study the universe using the infrared, ultra-violet, microwave, and radio parts of the spectrum. Light from each part of the spectrum carries different information with it, and can be used to study a different physical phenomenon. The drawback to this breadth of information is that every part of the spectrum needs its own dedicated detector, and often, whole telescope. Nevertheless, each wavelength of light reveals a part of the Universe hidden to us in all other wavelengths. 

Radio Astronomy was born in the 1930s with Karl Jansky's experiments with a directed 30 meter radio antenna. With it, he was able to detect thunderstorms, the Sun's magnetosphere, but also a strange unknown source at the center of our Galaxy. These experiments proved that low-frequency radio waves can be used to study the distant Universe. In the 1940s, radar receivers were developed as a matter of national security during the Second World War. They are considered the main reason for the victory in the Battle of Britain over the Luftwaffe. After the conclusion of the war, much of the radar hardware was turned to the skies. While the new hardware was much more sensitive to radio frequencies, radio telescopes have a fundamental limitation in terms of resolution. 

Because light behaves as a wave, especially at long wavelengths, it is bound by the diffraction limit. The diffraction limit links the smallest object detectable by a telescope with the wavelength of light and the telescope's diameter. If we take a 100-m radio telescope, it will have the resolving accuracy equivalent to a visible telescope with a mirror with a 0.5 mm diameter. This shouldn't be too shocking: radio waves are more than 10 million times longer than the light we perceive, so naturally our telescopes need to be 10 million times larger. 

To match the resolution of the Hubble space telescope at low frequencies, a telescope's mirror would need a diameter of 4300km. While engineers are still working on making radio dishes of this size, astronomers have turned to computing in order to increase the angular resolution of radio telescopes. Using the wave properties of light, radio astronomers can synchronize the data observed at multiple antenas, separated by tens, hundreds or thousands of kilometers. With significant processing, these distant antennas can be combined together to make an image with resolution equivalent to a radio telescope with a dish diameter of hundreds of kilometers. The downside of this combination of antennas is the computational requirements of making a scientific image. 

The LOFAR (LOw-Frequency-ARray) radio telescope is a Dutch low-frequency telescope that consists of more than 5000 antennas in the Netherlands with thousdans more in stations across Europe. LOFAR was designed to collect data for multiple science cases. As such, LOFAR data is stored at high resolutions, leading to large data sizes. Each 8-hour observation can fill eight 2-TB hard drives. Creating one image from this data is just possible on one's personal computer, however large all-sky surveys consist of thousands of observations and cannot be processed on a single computer or even a small cluster of computers. Even so, each run can take several days, a latency not feasible for projects producing several thousand data sets.

In order to make it possible to process thousands of multi-petabyte observations, we take advantage of data parallelism possible for processing of radio astronomy data. Data parallelism means that all of our data can be split into many different pieces, each of which can be processed independently. With sufficient number of workers, the processing time can drop significantly. Accelerating the first few processing steps delivers a second advantage: The initial steps take high resolution data and average it down by a factor of up to 64. Suddenly each observation can comfortably sit on your desktop, laptop and even on a micro-SD card! They can also be transported between universities in seconds rather than hours.


