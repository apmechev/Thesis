\cleardoublepage
\phantomsection
 
\addcontentsline{toc}{chapter}{English Summary}

\chapter*{English Summary}\label{ch:summary_eng}
When we look up at the night sky, we observe the universe through visible light. We can see stars, planets, nebulae and maybe even other galaxies. The earliest astronomers studied the uiniverse through their eyes, eventually aided by mirrors and lenses in the form of telescopes and binoculars. In the 18th and 19th centuries, physicists have unraveled the breadth of the electromagnetic spectrum. Aside from visible light, astronomers were now able to study the universe using the infrared, ultra-violet, microwave, and radio parts of the spectrum. Light from each part of the spectrum carries different information with it, and can be used to study a different physical phenomenon. The drawback to this breadth of information is that every part of the spectrum needs its own dedicated detector, and often, whole telescope. Nevertheless, each wavelength of light reveals a part of the Universe hidden to us in all other wavelengths. 

Radio Astronomy was born in the 1930s with Karl Jansky's experiments with a directed 30 meter radio antenna. With it, he was able to detect thunderstorms, the Sun's magnetosphere, but also a strange unknown source at the center of our Galaxy. These experiments proved that low-frequency radio waves can be used to study the distant Universe. In the 1940s, radar receivers were developed as a matter of national security during the Second World War. They are considered the main reason for the victory in the Battle of Britain over the Luftwaffe. After the conclusion of the war, much of the radar hardware was turned to the skies. While the new hardware was much more sensitive to radio frequencies, radio telescopes have a fundamental limitation in terms of resolution. 

Because light behaves as a wave, especially at long wavelengths, it is bound by the diffraction limit. The diffraction limit links the smallest object detectable by a telescope with the wavelength of light and the telescope's diameter. If we take a 100-m radio telescope, it will have the resolving accuracy equivalent to a visible telescope with a mirror with a 0.5 mm diameter. This shouldn't be too shocking: radio waves are more than 10 million times longer than the light we perceive, so naturally our telescopes need to be 10 million times larger. 

To match the resolution of the Hubble space telescope at low frequencies, a telescope's mirror would need a diameter of 4300km. While engineers are still working on making radio dishes of this size, astronomers have turned to computing in order to increase the angular resolution of radio telescopes. Using the wave properties of light, radio astronomers can synchronize the data observed at multiple antenas, separated by tens, hundreds or thousands of kilometers. With significant processing, these distant antennas can be combined together to make an image with resolution equivalent to a radio telescope with a dish diameter of hundreds of kilometers. The downside of this combination of antennas is the computational requirements of making a scientific image. 

The LOFAR (LOw-Frequency-ARray) radio telescope is a Dutch low-frequency telescope that consists of more than 5000 antennas in the Netherlands with thousdans more in stations across Europe. LOFAR was designed to collect data for multiple science cases. As such, LOFAR data is stored at high resolutions, leading to large data sizes. Each 8-hour observation can fill eight 2-TB hard drives. Creating one image from this data is just possible on one's personal computer, however large all-sky surveys consist of thousands of observations and cannot be processed on a single computer or even a small cluster of computers. Even so, each run can take several days, a latency not feasible for projects producing several thousand data sets.

In order to make it possible to process thousands of multi-petabyte observations, we take advantage of data parallelism possible for processing of radio astronomy data. Data parallelism means that all of our data can be split into many different pieces, each of which can be processed independently. With sufficient number of workers, the processing time can drop significantly. Accelerating the first few processing steps delivers a second advantage: The initial steps take high resolution data and average it down by a factor of up to 64. Suddenly each observation can comfortably sit on your desktop, laptop and even on a micro-SD card! They can also be transported between universities in seconds rather than hours. This work is focused on how to enable scientists to easily and quickly process LOFAR data, making it easier to use large data sters to conduct scientific studies.

In Chapter 1, we give an overview of the history of scientific computing, radio interferometry and the processing challenges for LOFAR data. In Chapter two, we present a platform for large scale distributed LOFAR processing. This platform is built for advanced LOFAR users who wish to parallelize their processing on a shared system distributed across multiple computational nodes and clusters. We present the software packages used and their interactions, and discuss the necessity of such a platform for current and future large scale astronomical studies. In Chapter three, we introduce a framework for launching, tracking and parallelizing processing jobs for the LOFAR telescope. Our results were the first time LOFAR data was processed in bulk, on a High Throughput Infrastructure. We show the applicability of this computing paradigm on the initial steps of a single processing pipeline. We suggest how other pipelines and telescopes can use this framework to process their data.  Finally, we show a significant speed-up in data processing compared to data processing in Leiden: up to 35 times faster. 

In Chapter four, we tackle the challenge of collecting performance data from distributed runs of a complex pipeline. Building on our work parallelizing processing pipelines, we build helper software to track the performance of each processing step. This data is collected at a central server and can be analysed in real time or studied after the processing run. We run tests on four different systems and discover that the way the software is built doesn't degrade performance. This discovery is important, since it means we can distribute software without a performance penalty. We also gain insights into low-level performance for our slowest steps.  In chapter five, we introduce the first workflow orchestration software for complex LOFAR pipelines. This software is suited best for automatic processing of LOFAR data produced by large, long-running projects. 

