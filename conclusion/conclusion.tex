\chapter{Conclusion}

\label{ch:conclusions}

As astronomical observatories collect ever growing data-sets, the processing challenges for these data will continue increasing. Large scale surveys expected to produce petabytes of data can no longer be processed on single machine or small dedicated clusters at scientific institutions. Large scale distributed processing is needed to serve the scientific requirements of these survey projects. 

CERN's World-Wide computing grid provides sufficent resources for such projects, however due to its focus on distributed Monte-Carlo simulations, it also presents some design constraints. Namely, porting complex workflows to a grid-like environment requires a framework to distribute and monitor jobs. Additionally, a workflow orchestration software is needed to schedule and automate processing. 

\section{Summary of Thesis Achievements}

This work focuses on the software built to accelerate, parallelize, and automate LOFAR processing,  as well as the insights obtained into large scale processing of LOFAR data. To date, we have helped process an unprecedented 8 petabytes of data for the LOFAR Two-Meter Sky Survey (LoTSS), data which has led to  more than 30 publications. We describe a generic platform for scaling astronomical processing across multiple clusters, focused on the  application of bulk LOFAR processing.  

We have built software that can encapsulate LOFAR processing steps and distribute them acros a heterogenous infrastructure. Our tools have been used by several scientists, implementing multiple complex pipelines, processing a total of 8 petabytes of data. 

We implemented a complex monitoring suite along our processing to track the performance of individual pipeline steps. 

\section{Answers to Research Questions}


\begin{addmargin}[4em]{8em}% 1em left, 2em right
    \emph{\textbf{Research Question 1:} How can we use a distributed shared infrastructure for efficient LOFAR data processing?}
\end{addmargin}

In Chapters \ref{ch:LOFAR_DSP} and \ref{ch:GRID_LRT}, we detail our success with massively distributed processing of LOFAR data. We describe the underlying platform, inherited from the High Energy Physics community and the modifications to these tools that were required to host complex processing software. We detail these modifications and discuss the increas in throughput that distributed processing leads to. Finally, we make estimations on the processing time saved by parallelizing LOFAR data processing. The work described in these chapters is essential to producing scientific data sets at a high cadence, particularly considering the high data rates produced by LOFAR.  


\begin{addmargin}[4em]{8em}% 1em left, 2em right
    \emph{\textbf{Research Question 2:} How can we build software to easily accelerate complex pipelines for Radio Astronomy?}
\end{addmargin}

Chapters \ref{ch:AGLOW} and \ref{ch:AGLOW_CI} detail the advances in parallelizing complex scientific pipelines on a distributed shared infrastructures. We integrate a mature workflow orchestration package with distributed LOFAR processing. We discuss the need for this orchestration, as well as the abilities to support additional complex pipelines. As an example application, we build a Continuous Integration pipeline tasked with verifying and validating the initial steps of LOFAR processing. 


\begin{addmargin}[4em]{8em}% 1em left, 2em right
    \emph{\textbf{Research Question 3:} Can we automatically collect performance information during massively distributed processing and predict run times for future data sets?}
\end{addmargin}

Chapters \ref{ch:pipeline_collector} and \ref{ch:Scalability_model} describe a performance monitoring suite for LOFAR data and our scalability  model for LOFAR processing. When running massively distributed processing, scientists are unable to monitor the performance of the underlying software. Collecting these statistics is necessary for understanding processing inefficiencies and suggest ways to accelerate data processing. Performance data can also be used to understand the effect of processing parameters on the resource usage of complex pipelines. We study this in detail, building a model that can be used to understand the scalability of multiple processing steps. This model shows the limitations on scientific parameters imposed by limited processing resources as well as suggestions on decreasing processing time without sacrificing scientific data quality. 

\section{Limitations}

Using the software described, the LOFAR Surveys team was able to process several petabytes of archived data and produce scientific quality images. Despite the successes of the project, there are several issues that occasionally impede data processing and prevent rapid deployment of software pipelines.

The primary issue is difficulty deploying processing software on the restricted computational clusters at Forschungszentrum J{\:u}lich, one of the three LOFAR archive locations. As this cluster neither supports docker, singularity nor CVMFS, deploying new software is difficult and time consuming. Additionally, orchestrating jobs at this site requires extra integration with our tools.

Furthermore, our tools do not include data quality checks and automatic reprocessing, meaning some user overhead is needed to check the quality of each dataset and to debug common processing errors. As the software and scientific pipelines mature, we expect this issue to be slowly resolved.

Finally, our current software distribution does not give semantic version to software images, nor is there a way to store these images or cite them in related papers. Implementing these features will not only make data processing easily reproducible but also make it possible to recognize the effort put into building and distributing software images.


\section{Future Work}

This work focuses on the first stages of LOFAR data processing, because of the large gains possible by parallelization. We take in mind the complexity of our processing workflows, the wide range of scientific pipelines and the heterogenous nature of the underlying infrastructure. Because of these factors, the software we've built can be used by a wide range of astronomical pipelines. Moreover, we can incorporate processing hosted at scientific institutions and cloud providers, to scale scientific processing horizontally.  One application for large scale distributed processing is the Square kilometer Array. 

The Square Kilometer Array, (SKA) is a planned aperture synthesis radio telescope expected to have a total collecting area of one square kilometer. It is expected to produce more than 160 TB per day \citep{johnston2017taming}, data which needs to be promptly processed. Scaling our tools to SKA-size processing requires a federation of clusters able to handle a high throughput workload.   
